srun: Warning: can't run 4 processes on 16 nodes, setting nnodes to 4
0: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid001312', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Amazon_Large_4/processed/partitions.txt')
0: hostname: nid001312 rank: 0 size: 4
1: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid001312', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Amazon_Large_4/processed/partitions.txt')
1: hostname: nid001684 rank: 1 size: 4
2: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid001312', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Amazon_Large_4/processed/partitions.txt')
2: hostname: nid002181 rank: 2 size: 4
3: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid001312', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Amazon_Large_4/processed/partitions.txt')
3: hostname: nid002657 rank: 3 size: 4
1: 1
1: Loading coo...
2: 2
2: Loading coo...
3: 3
3: Loading coo...
0: 0
0: Loading coo...
1: Done loading coo
0: Done loading coo
2: Done loading coo
3: Done loading coo
1: 3494515
1: 3630304
1: 3405111
1: 3719709
1: partitioning...
0: 3494515
0: 3630304
0: 3405111
0: 3719709
0: partitioning...
2: 3494515
2: 3630304
2: 3405111
2: 3719709
2: partitioning...
3: 3494515
3: 3630304
3: 3405111
3: 3719709
3: partitioning...
3: rank: 3 adj_matrix_loc.size: torch.Size([14249639, 3719709])
3: rank: 3 inputs.size: torch.Size([14249639, 300])
3: done partitioning
0: rank: 0 adj_matrix_loc.size: torch.Size([14249639, 3494515])
0: rank: 0 inputs.size: torch.Size([14249639, 300])
0: done partitioning
2: rank: 2 adj_matrix_loc.size: torch.Size([14249639, 3405111])
2: rank: 2 inputs.size: torch.Size([14249639, 300])
2: done partitioning
1: rank: 1 adj_matrix_loc.size: torch.Size([14249639, 3630304])
1: rank: 1 inputs.size: torch.Size([14249639, 300])
1: done partitioning
2: tensor(3494513, device='cuda:2')
2: torch.Size([3405111, 3494515])
2: tensor(3630303, device='cuda:2')
2: torch.Size([3405111, 3630304])
2: tensor(3405110, device='cuda:2')
2: torch.Size([3405111, 3405111])
2: tensor(3719708, device='cuda:2')
2: torch.Size([3405111, 3719709])
2: Epoch: 0
1: tensor(3494513, device='cuda:1')
1: torch.Size([3630304, 3494515])
1: tensor(3630303, device='cuda:1')
1: torch.Size([3630304, 3630304])
1: tensor(3405110, device='cuda:1')
1: torch.Size([3630304, 3405111])
1: tensor(3719708, device='cuda:1')
1: torch.Size([3630304, 3719709])
1: Epoch: 0
3: tensor(3494513, device='cuda:3')
3: torch.Size([3719709, 3494515])
3: tensor(3630303, device='cuda:3')
3: torch.Size([3719709, 3630304])
3: tensor(3405110, device='cuda:3')
3: torch.Size([3719709, 3405111])
3: tensor(3719708, device='cuda:3')
3: torch.Size([3719709, 3719709])
3: Epoch: 0
0: tensor(3494513, device='cuda:0')
0: torch.Size([3494515, 3494515])
0: tensor(3630303, device='cuda:0')
0: torch.Size([3494515, 3630304])
0: tensor(3405110, device='cuda:0')
0: torch.Size([3494515, 3405111])
0: tensor(3719708, device='cuda:0')
0: torch.Size([3494515, 3719709])
0: Epoch: 0
2: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
2:   warnings.warn(
0: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
0:   warnings.warn(
1: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
1:   warnings.warn(
3: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
3:   warnings.warn(
2: Epoch: 1
1: Epoch: 1
0: Epoch: 1
3: Epoch: 1
1: Traceback (most recent call last):
1:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 620, in <module>
1:     main(args)
1:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 532, in main
1:     logits = model(g_loc, features_loc, ampbyp, epoch)
1:   File "/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
1:     return forward_call(*input, **kwargs)
1:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 51, in forward
1:     h = layer(self, graph, h, ampbyp)
1:   File "/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
1:     return forward_call(*input, **kwargs)
1:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 727, in forward
1:     return GCNFuncONED.apply(gcn, graph, ampbyp, inputs, self.weight)
1:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 743, in forward
1:     z = broad_func_oned(self, graph, ampbyp, inputs)
1:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 98, in broad_func_oned
1:     spmm_gpu(ampbyp[i].indices()[0].int(), ampbyp[i].indices()[1].int(),
1: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.06 GiB (GPU 1; 39.45 GiB total capacity; 30.50 GiB already allocated; 3.57 GiB free; 34.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
3: Traceback (most recent call last):
3:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 620, in <module>
3:     main(args)
3:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 532, in main
3:     logits = model(g_loc, features_loc, ampbyp, epoch)
3:   File "/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
3:     return forward_call(*input, **kwargs)
3:   File "/global/u2/j/jinimukh/CAGNET/examples/gcn_1d.py", line 51, in forward
3:     h = layer(self, graph, h, ampbyp)
3:   File "/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
3:     return forward_call(*input, **kwargs)
3:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 727, in forward
3:     return GCNFuncONED.apply(gcn, graph, ampbyp, inputs, self.weight)
3:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 743, in forward
3:     z = broad_func_oned(self, graph, ampbyp, inputs)
3:   File "/global/homes/j/jinimukh/CAGNET/cagnet/nn/conv/gcn_conv.py", line 98, in broad_func_oned
3:     spmm_gpu(ampbyp[i].indices()[0].int(), ampbyp[i].indices()[1].int(),
3: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 3; 39.45 GiB total capacity; 31.02 GiB already allocated; 2.99 GiB free; 34.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: nid001684: task 1: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=6051853.0
0: slurmstepd: error: *** STEP 6051853.0 ON nid001312 CANCELLED AT 2023-03-14T05:48:46 ***
srun: error: nid002657: task 3: Exited with exit code 1
srun: error: nid002181: task 2: Terminated
srun: error: nid001312: task 0: Terminated
srun: Force Terminated StepId=6051853.0
