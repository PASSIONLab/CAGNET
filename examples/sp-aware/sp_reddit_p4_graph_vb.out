srun: Warning: can't run 4 processes on 16 nodes, setting nnodes to 4
0: Namespace(dataset='Reddit_4_graph_vb', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002164', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Reddit_4_graph_vb/processed/partitions.txt')
0: hostname: nid002164 rank: 0 size: 4
1: Namespace(dataset='Reddit_4_graph_vb', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002164', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Reddit_4_graph_vb/processed/partitions.txt')
1: hostname: nid002165 rank: 1 size: 4
2: Namespace(dataset='Reddit_4_graph_vb', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002164', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Reddit_4_graph_vb/processed/partitions.txt')
2: hostname: nid002168 rank: 2 size: 4
3: Namespace(dataset='Reddit_4_graph_vb', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002164', normalize=False, partitioning='ONED', timers=True, partitions='/pscratch/sd/j/jinimukh/Reddit_4_graph_vb/processed/partitions.txt')
3: hostname: nid002169 rank: 3 size: 4
0: 0
0: Loading coo...
1: 1
1: Loading coo...
2: 2
2: Loading coo...
3: 3
3: Loading coo...
0: Done loading coo
1: Done loading coo
2: Done loading coo
3: Done loading coo
0: 55786
0: 60696
0: 57151
0: 59332
0: partitioning...
3: 55786
3: 60696
3: 57151
3: 59332
3: partitioning...
1: 55786
1: 60696
1: 57151
1: 59332
1: partitioning...
2: 55786
2: 60696
2: 57151
2: 59332
2: partitioning...
0: rank: 0 adj_matrix_loc.size: torch.Size([232965, 55786])
0: rank: 0 inputs.size: torch.Size([232965, 602])
0: done partitioning
1: rank: 1 adj_matrix_loc.size: torch.Size([232965, 60696])
1: rank: 1 inputs.size: torch.Size([232965, 602])
1: done partitioning
3: rank: 3 adj_matrix_loc.size: torch.Size([232965, 59332])
3: rank: 3 inputs.size: torch.Size([232965, 602])
3: done partitioning
2: rank: 2 adj_matrix_loc.size: torch.Size([232965, 57151])
2: rank: 2 inputs.size: torch.Size([232965, 602])
2: done partitioning
0: tensor(55785, device='cuda:0')
0: torch.Size([55786, 55786])
0: tensor(60690, device='cuda:0')
0: torch.Size([55786, 60696])
0: tensor(57150, device='cuda:0')
0: torch.Size([55786, 57151])
0: tensor(59331, device='cuda:0')
0: torch.Size([55786, 59332])
0: Epoch: 0
1: tensor(55785, device='cuda:1')
1: torch.Size([60696, 55786])
1: tensor(60695, device='cuda:1')
1: torch.Size([60696, 60696])
1: tensor(57150, device='cuda:1')
1: torch.Size([60696, 57151])
1: tensor(59330, device='cuda:1')
1: torch.Size([60696, 59332])
1: Epoch: 0
2: tensor(55785, device='cuda:2')
2: torch.Size([57151, 55786])
2: tensor(60695, device='cuda:2')
2: torch.Size([57151, 60696])
2: tensor(57150, device='cuda:2')
2: torch.Size([57151, 57151])
2: tensor(59331, device='cuda:2')
2: torch.Size([57151, 59332])
2: Epoch: 0
3: tensor(55781, device='cuda:3')
3: torch.Size([59332, 55786])
3: tensor(60695, device='cuda:3')
3: torch.Size([59332, 60696])
3: tensor(57150, device='cuda:3')
3: torch.Size([59332, 57151])
3: tensor(59331, device='cuda:3')
3: torch.Size([59332, 59332])
3: Epoch: 0
0: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
0:   warnings.warn(
1: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
1:   warnings.warn(
3: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
3:   warnings.warn(
2: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
2:   warnings.warn(
0: Epoch: 1
1: Epoch: 1
3: Epoch: 1
2: Epoch: 1
3: Epoch: 2
0: Epoch: 2
1: Epoch: 2
2: Epoch: 2
3: Epoch: 3
0: Epoch: 3
1: Epoch: 3
2: Epoch: 3
3: Epoch: 4
0: Epoch: 4
1: Epoch: 4
2: Epoch: 4
3: Epoch: 5
0: Epoch: 5
1: Epoch: 5
2: Epoch: 5
3: Epoch: 6
0: Epoch: 6
1: Epoch: 6
2: Epoch: 6
3: Epoch: 7
0: Epoch: 7
1: Epoch: 7
2: Epoch: 7
3: Epoch: 8
0: Epoch: 8
1: Epoch: 8
2: Epoch: 8
3: Epoch: 9
0: Epoch: 9
1: Epoch: 9
2: Epoch: 9
3: Epoch: 10
0: Epoch: 10
1: Epoch: 10
2: Epoch: 10
3: Epoch: 11
0: Epoch: 11
1: Epoch: 11
2: Epoch: 11
3: Epoch: 12
0: Epoch: 12
1: Epoch: 12
2: Epoch: 12
3: Epoch: 13
0: Epoch: 13
1: Epoch: 13
2: Epoch: 13
3: Epoch: 14
0: Epoch: 14
1: Epoch: 14
2: Epoch: 14
3: Epoch: 15
0: Epoch: 15
1: Epoch: 15
2: Epoch: 15
3: Epoch: 16
0: Epoch: 16
1: Epoch: 16
2: Epoch: 16
3: Epoch: 17
0: Epoch: 17
1: Epoch: 17
2: Epoch: 17
3: Epoch: 18
0: Epoch: 18
1: Epoch: 18
2: Epoch: 18
3: Epoch: 19
0: Epoch: 19
1: Epoch: 19
2: Epoch: 19
3: Epoch: 20
0: Epoch: 20
1: Epoch: 20
2: Epoch: 20
3: Epoch: 21
0: Epoch: 21
1: Epoch: 21
2: Epoch: 21
3: Epoch: 22
0: Epoch: 22
1: Epoch: 22
2: Epoch: 22
3: Epoch: 23
0: Epoch: 23
1: Epoch: 23
2: Epoch: 23
3: Epoch: 24
0: Epoch: 24
1: Epoch: 24
2: Epoch: 24
3: Epoch: 25
0: Epoch: 25
1: Epoch: 25
2: Epoch: 25
3: Epoch: 26
0: Epoch: 26
1: Epoch: 26
2: Epoch: 26
3: Epoch: 27
0: Epoch: 27
1: Epoch: 27
2: Epoch: 27
3: Epoch: 28
0: Epoch: 28
1: Epoch: 28
2: Epoch: 28
3: Epoch: 29
0: Epoch: 29
1: Epoch: 29
2: Epoch: 29
3: Epoch: 30
0: Epoch: 30
1: Epoch: 30
2: Epoch: 30
3: Epoch: 31
0: Epoch: 31
1: Epoch: 31
2: Epoch: 31
0: Epoch: 32
3: Epoch: 32
1: Epoch: 32
2: Epoch: 32
3: Epoch: 33
0: Epoch: 33
1: Epoch: 33
2: Epoch: 33
3: Epoch: 34
0: Epoch: 34
1: Epoch: 34
2: Epoch: 34
3: Epoch: 35
0: Epoch: 35
1: Epoch: 35
2: Epoch: 35
3: Epoch: 36
0: Epoch: 36
1: Epoch: 36
2: Epoch: 36
3: Epoch: 37
0: Epoch: 37
1: Epoch: 37
2: Epoch: 37
3: Epoch: 38
0: Epoch: 38
1: Epoch: 38
2: Epoch: 38
3: Epoch: 39
0: Epoch: 39
1: Epoch: 39
2: Epoch: 39
3: Epoch: 40
0: Epoch: 40
1: Epoch: 40
2: Epoch: 40
3: Epoch: 41
0: Epoch: 41
1: Epoch: 41
2: Epoch: 41
3: Epoch: 42
0: Epoch: 42
1: Epoch: 42
2: Epoch: 42
3: Epoch: 43
0: Epoch: 43
1: Epoch: 43
2: Epoch: 43
3: Epoch: 44
0: Epoch: 44
1: Epoch: 44
2: Epoch: 44
3: Epoch: 45
0: Epoch: 45
1: Epoch: 45
2: Epoch: 45
3: Epoch: 46
0: Epoch: 46
1: Epoch: 46
2: Epoch: 46
3: Epoch: 47
0: Epoch: 47
1: Epoch: 47
2: Epoch: 47
3: Epoch: 48
0: Epoch: 48
1: Epoch: 48
2: Epoch: 48
3: Epoch: 49
0: Epoch: 49
1: Epoch: 49
2: Epoch: 49
3: Epoch: 50
0: Epoch: 50
1: Epoch: 50
2: Epoch: 50
3: Epoch: 51
0: Epoch: 51
1: Epoch: 51
2: Epoch: 51
3: Epoch: 52
0: Epoch: 52
1: Epoch: 52
2: Epoch: 52
3: Epoch: 53
0: Epoch: 53
1: Epoch: 53
2: Epoch: 53
3: Epoch: 54
0: Epoch: 54
1: Epoch: 54
2: Epoch: 54
3: Epoch: 55
0: Epoch: 55
1: Epoch: 55
2: Epoch: 55
3: Epoch: 56
0: Epoch: 56
1: Epoch: 56
2: Epoch: 56
3: Epoch: 57
0: Epoch: 57
1: Epoch: 57
2: Epoch: 57
3: Epoch: 58
0: Epoch: 58
1: Epoch: 58
2: Epoch: 58
3: Epoch: 59
0: Epoch: 59
1: Epoch: 59
2: Epoch: 59
3: Epoch: 60
0: Epoch: 60
1: Epoch: 60
2: Epoch: 60
3: Epoch: 61
0: Epoch: 61
1: Epoch: 61
2: Epoch: 61
3: Epoch: 62
0: Epoch: 62
1: Epoch: 62
2: Epoch: 62
3: Epoch: 63
0: Epoch: 63
1: Epoch: 63
2: Epoch: 63
3: Epoch: 64
0: Epoch: 64
1: Epoch: 64
2: Epoch: 64
3: Epoch: 65
0: Epoch: 65
1: Epoch: 65
2: Epoch: 65
3: Epoch: 66
0: Epoch: 66
1: Epoch: 66
2: Epoch: 66
3: Epoch: 67
0: Epoch: 67
1: Epoch: 67
2: Epoch: 67
3: Epoch: 68
0: Epoch: 68
1: Epoch: 68
2: Epoch: 68
3: Epoch: 69
0: Epoch: 69
1: Epoch: 69
2: Epoch: 69
3: Epoch: 70
0: Epoch: 70
1: Epoch: 70
2: Epoch: 70
3: Epoch: 71
0: Epoch: 71
1: Epoch: 71
2: Epoch: 71
3: Epoch: 72
0: Epoch: 72
1: Epoch: 72
2: Epoch: 72
3: Epoch: 73
0: Epoch: 73
1: Epoch: 73
2: Epoch: 73
3: Epoch: 74
0: Epoch: 74
1: Epoch: 74
2: Epoch: 74
3: Epoch: 75
0: Epoch: 75
1: Epoch: 75
2: Epoch: 75
3: Epoch: 76
0: Epoch: 76
1: Epoch: 76
2: Epoch: 76
3: Epoch: 77
0: Epoch: 77
1: Epoch: 77
2: Epoch: 77
3: Epoch: 78
0: Epoch: 78
1: Epoch: 78
2: Epoch: 78
3: Epoch: 79
0: Epoch: 79
1: Epoch: 79
2: Epoch: 79
3: Epoch: 80
0: Epoch: 80
1: Epoch: 80
2: Epoch: 80
3: Epoch: 81
0: Epoch: 81
1: Epoch: 81
2: Epoch: 81
3: Epoch: 82
0: Epoch: 82
1: Epoch: 82
2: Epoch: 82
3: Epoch: 83
0: Epoch: 83
1: Epoch: 83
2: Epoch: 83
3: Epoch: 84
0: Epoch: 84
1: Epoch: 84
2: Epoch: 84
3: Epoch: 85
0: Epoch: 85
1: Epoch: 85
2: Epoch: 85
3: Epoch: 86
0: Epoch: 86
1: Epoch: 86
2: Epoch: 86
3: Epoch: 87
0: Epoch: 87
1: Epoch: 87
2: Epoch: 87
3: Epoch: 88
0: Epoch: 88
1: Epoch: 88
2: Epoch: 88
3: Epoch: 89
0: Epoch: 89
1: Epoch: 89
2: Epoch: 89
3: Epoch: 90
0: Epoch: 90
1: Epoch: 90
2: Epoch: 90
3: Epoch: 91
0: Epoch: 91
1: Epoch: 91
2: Epoch: 91
3: Epoch: 92
0: Epoch: 92
1: Epoch: 92
2: Epoch: 92
3: Epoch: 93
0: Epoch: 93
1: Epoch: 93
2: Epoch: 93
3: Epoch: 94
0: Epoch: 94
1: Epoch: 94
2: Epoch: 94
3: Epoch: 95
0: Epoch: 95
1: Epoch: 95
2: Epoch: 95
3: Epoch: 96
0: Epoch: 96
1: Epoch: 96
2: Epoch: 96
3: Epoch: 97
0: Epoch: 97
1: Epoch: 97
2: Epoch: 97
3: Epoch: 98
0: Epoch: 98
1: Epoch: 98
2: Epoch: 98
3: Epoch: 99
0: Epoch: 99
1: Epoch: 99
2: Epoch: 99
0: rank: 0 total_time: 19.36394739151001
0: rank: 0 timings: defaultdict(<class 'float'>, {'a2a2': 0.005384206771850586, 'gather_row_data': 0.11250519752502441, 'a2a3': 1.6257622241973877, 'spmm_gpu': 10.071173429489136})
1: rank: 1 total_time: 19.359103441238403
1: rank: 1 timings: defaultdict(<class 'float'>, {'a2a2': 0.0047588348388671875, 'gather_row_data': 0.11301159858703613, 'a2a3': 1.411630630493164, 'spmm_gpu': 11.410393476486206})
3: rank: 3 total_time: 19.35742425918579
3: rank: 3 timings: defaultdict(<class 'float'>, {'a2a2': 0.005319118499755859, 'gather_row_data': 0.12164473533630371, 'a2a3': 1.7275011539459229, 'spmm_gpu': 10.022805452346802})
2: rank: 2 total_time: 19.354910612106323
2: rank: 2 timings: defaultdict(<class 'float'>, {'a2a2': 0.0002512931823730469, 'gather_row_data': 0.11994528770446777, 'a2a3': 1.751344919204712, 'spmm_gpu': 17.099412202835083})
