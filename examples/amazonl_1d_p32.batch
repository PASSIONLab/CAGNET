#!/bin/bash
#SBATCH -A m1982_g
#SBATCH -C gpu
#SBATCH --job-name="job-1d-Amazon-Large"
#SBATCH -q regular
#SBATCH -t 20:00
#SBATCH -n 32
#SBATCH -N 8


export MASTER_ADDR=$(scontrol show hostnames | head -n 1)

export PYTHONPATH="/global/u2/j/jinimukh/CAGNET/sparse-extension/build/lib/python3.9/site-packages":$PYTHONPATH
export PYTHONPATH="/global/u2/j/jinimukh/CAGNET/sparse-extension/build/lib/python3.9/site-packages/sparse_coo_tensor_cpp-0.0.0-py3.9-linux-x86_64.egg":$PYTHONPATH

export CFLAGS="-I$CUDA_HOME/include"
module load pytorch/1.11


srun -l -n 32 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR  &> sp_amazonl_p32.out

srun -l -n 32 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large_32 --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR --partitions=/pscratch/sd/j/jinimukh/Amazon_Large_32/processed/partitions.txt &> sp_amazonl_p32_gvb.out


#srun -l -n 4 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR &> sp_amazonl_p4.out

#srun -l -n 16 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR &> sp_amazonl_p16.out

#srun -l -n 32 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR &> sp_amazonl_p32.out

#srun -l -n 64 --cpus-per-task 32 --ntasks-per-node 4 --gpus-per-node 4 python3 gcn_1d.py --dataset Amazon_Large --n-epochs 100 --n-hidden 16 --weight-decay 0 --timers --hostname=$MASTER_ADDR &> sp_amazonl_p64.out
