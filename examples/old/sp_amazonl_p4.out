srun: Warning: can't run 4 processes on 16 nodes, setting nnodes to 4
0: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002617', normalize=False, partitioning='ONED', timers=True, partitions='')
0: hostname: nid002617 rank: 0 size: 4
1: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002617', normalize=False, partitioning='ONED', timers=True, partitions='')
1: hostname: nid002620 rank: 1 size: 4
2: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002617', normalize=False, partitioning='ONED', timers=True, partitions='')
2: hostname: nid002621 rank: 2 size: 4
3: Namespace(dataset='Amazon_Large', dropout=0.5, gpu=4, lr=0.01, n_epochs=100, n_hidden=16, n_layers=1, weight_decay=0.0, aggregator_type='gcn', world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', hostname='nid002617', normalize=False, partitioning='ONED', timers=True, partitions='')
3: hostname: nid002624 rank: 3 size: 4
2: 2
2: Loading coo...
1: 1
1: Loading coo...
3: 3
3: Loading coo...
0: 0
0: Loading coo...
2: Done loading coo
1: Done loading coo
0: Done loading coo
3: Done loading coo
2: partitioning...
1: partitioning...
3: partitioning...
0: partitioning...
2: rank: 2 adj_matrix_loc.size: torch.Size([14249639, 3562410])
2: rank: 2 inputs.size: torch.Size([14249639, 300])
2: done partitioning
3: rank: 3 adj_matrix_loc.size: torch.Size([14249639, 3562409])
3: rank: 3 inputs.size: torch.Size([14249639, 300])
3: done partitioning
0: rank: 0 adj_matrix_loc.size: torch.Size([14249639, 3562410])
0: rank: 0 inputs.size: torch.Size([14249639, 300])
0: done partitioning
1: rank: 1 adj_matrix_loc.size: torch.Size([14249639, 3562410])
1: rank: 1 inputs.size: torch.Size([14249639, 300])
1: done partitioning
3: tensor(3562405, device='cuda:3')
3: torch.Size([3562409, 3562410])
3: tensor(3562409, device='cuda:3')
3: torch.Size([3562409, 3562410])
3: tensor(3562401, device='cuda:3')
3: torch.Size([3562409, 3562410])
3: tensor(3562408, device='cuda:3')
3: torch.Size([3562409, 3562409])
3: Epoch: 0
2: tensor(3562403, device='cuda:2')
2: torch.Size([3562410, 3562410])
2: tensor(3562409, device='cuda:2')
2: torch.Size([3562410, 3562410])
2: tensor(3562408, device='cuda:2')
2: torch.Size([3562410, 3562410])
2: tensor(3562408, device='cuda:2')
2: torch.Size([3562410, 3562409])
2: Epoch: 0
0: tensor(3562408, device='cuda:0')
0: torch.Size([3562410, 3562410])
0: tensor(3562409, device='cuda:0')
0: torch.Size([3562410, 3562410])
0: tensor(3562405, device='cuda:0')
0: torch.Size([3562410, 3562410])
0: tensor(3562408, device='cuda:0')
0: torch.Size([3562410, 3562409])
0: Epoch: 0
1: tensor(3562408, device='cuda:1')
1: torch.Size([3562410, 3562410])
1: tensor(3562408, device='cuda:1')
1: torch.Size([3562410, 3562410])
1: tensor(3562408, device='cuda:1')
1: torch.Size([3562410, 3562410])
1: tensor(3562408, device='cuda:1')
1: torch.Size([3562410, 3562409])
1: Epoch: 0
3: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
3:   warnings.warn(
0: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
0:   warnings.warn(
2: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
2:   warnings.warn(
1: /global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
1:   warnings.warn(
0: Epoch: 1
1: Epoch: 1
3: Epoch: 1
2: Epoch: 1
3: Epoch: 2
1: Epoch: 2
2: Epoch: 2
0: Epoch: 2
3: Epoch: 3
1: Epoch: 3
2: Epoch: 3
0: Epoch: 3
3: Epoch: 4
0: Epoch: 4
1: Epoch: 4
2: Epoch: 4
3: Epoch: 5
0: Epoch: 5
1: Epoch: 5
2: Epoch: 5
3: Epoch: 6
0: Epoch: 6
1: Epoch: 6
2: Epoch: 6
3: Epoch: 7
0: Epoch: 7
1: Epoch: 7
2: Epoch: 7
3: Epoch: 8
1: Epoch: 8
2: Epoch: 8
0: Epoch: 8
3: Epoch: 9
0: Epoch: 9
1: Epoch: 9
2: Epoch: 9
3: Epoch: 10
0: Epoch: 10
1: Epoch: 10
2: Epoch: 10
0: Epoch: 11
3: Epoch: 11
1: Epoch: 11
2: Epoch: 11
3: Epoch: 12
0: Epoch: 12
1: Epoch: 12
2: Epoch: 12
3: Epoch: 13
1: Epoch: 13
2: Epoch: 13
0: Epoch: 13
3: Epoch: 14
0: Epoch: 14
1: Epoch: 14
2: Epoch: 14
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
0: slurmstepd: error: *** STEP 6053460.0 ON nid002617 CANCELLED AT 2023-03-14T07:24:41 ***
